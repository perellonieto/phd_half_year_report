

@inproceedings{perello2016,
  title={Background Check: A general technique to build more reliable and versatile classifiers},
  author={Perello-Nieto, Miquel and Telmo De Menezes Filho, E Silva and Kull, Meelis and Flach, Peter},
  booktitle={Data Mining (ICDM), 2016 IEEE 16th International Conference on},
  pages={1143--1148},
  year={2016},
  organization={IEEE}
}

@inproceedings{perello2017,
abstract = {In many real-world problems, labels are often weak, meaning that each instance is labelled as belonging to one of several candidate categories, at most one of them being true. Recent theoretical contributions have shown that it is possible to construct proper losses or classification calibrated losses for weakly labelled classification scenarios by means of a linear transformation of conventional proper or classification calibrated losses, respectively. However, how to translate these theoretical results into practice has not been explored yet. This paper discusses both the algorithmic design and the potential advantages of this approach, analyzing consistency and convexity issues arising in practical settings, and evaluating the behavior of such transformations under different types of weak labels.},
address = {London, UK},
author = {Perell{\'{o}}-Nieto, Miquel and Santos-Rodr{\'{i}}guez, Ra{\'{u}}l and Cid-Sueiro, Jes{\'{u}}s},
booktitle = {International Symposium on Intelligent Data Analysis},
doi = {10.1007/978-3-319-68765-0_21},
file = {:home/maikel/Documents/mendeley/Perell{\'{o}}-Nieto, Santos-Rodr{\'{i}}guez, Cid-Sueiro/International Symposium on Intelligent Data Analysis/Perell{\'{o}}-Nieto, Santos-Rodr{\'{i}}guez, Cid-Sueiro{\_}2017{\_}Adapting Supervised Classification Algorithms to Arbitrary Weak Label Scenarios.pdf:pdf},
keywords = {Noisy labels,Proper losses,Weak labels},
mendeley-groups = {Books},
month = {oct},
pages = {247--259},
publisher = {Springer, Cham},
title = {{Adapting Supervised Classification Algorithms to Arbitrary Weak Label Scenarios}},
url = {http://link.springer.com/10.1007/978-3-319-68765-0{\_}21},
year = {2017}
}

@book{Bishop2006,
author = {Bishop, Christopher M.},
file = {:home/maikel/Documents/mendeley/Bishop/Unknown/Bishop{\_}2006{\_}Pattern recognition and machine learning.pdf:pdf},
keywords = {mscthesis},
mendeley-groups = {mscthesis,Zotero - My Collection,Game theory,Zotero - Zotero Library,Books},
mendeley-tags = {mscthesis},
publisher = {New York: springer, 2006.},
title = {{Pattern recognition and machine learning.}},
volume = {1},
year = {2006}
}

@article{Naeini2015,
abstract = {Learning probabilistic predictive models that are well calibrated is critical for many prediction and decision-making tasks in artificial intelligence. In this paper we present a new non-parametric calibration method called Bayesian Binning into Quantiles (BBQ) which addresses key limitations of existing calibration methods. The method post processes the output of a binary classification algorithm; thus, it can be readily combined with many existing classification algorithms. The method is computationally tractable, and empirically accurate, as evidenced by the set of experiments reported here on both real and simulated datasets.},
author = {Naeini, Mahdi Pakdaman and Cooper, Gregory F and Hauskrecht, Milos},
file = {:home/maikel/Documents/mendeley/Naeini, Cooper, Hauskrecht/Proceedings of the ... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence/Naeini, Cooper, Hauskrecht{\_}2015{\_}Obtaining Well Calibrated Probabilities Using Bayesian Binning.pdf:pdf},
issn = {2159-5399},
journal = {Proceedings of the ... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence},
mendeley-groups = {UoB/bayesian{\_}calibration},
month = {jan},
pages = {2901--2907},
pmid = {25927013},
publisher = {NIH Public Access},
title = {{Obtaining Well Calibrated Probabilities Using Bayesian Binning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25927013 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4410090},
volume = {2015},
year = {2015}
}

@article{Zadrozny2001,
author = {Zadrozny, B and Elkan, C},
file = {:home/maikel/Documents/mendeley/Zadrozny, Elkan/ICML/Zadrozny, Elkan{\_}2001{\_}Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers.pdf:pdf},
journal = {ICML},
mendeley-groups = {REFRAME/multi-class calibration workteam,UoB/bayesian{\_}calibration},
title = {{Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.29.3039{\&}rep=rep1{\&}type=pdf},
year = {2001}
}

@article{Platt1999,
abstract = {The output of a classifier should be a calibrated posterior probability to enable post-processing. Standard SVMs do not provide such probabilities. One method to create probabilities is to directly train a kernel classifier with a logit link function and a regularized maximum likelihood score. However, training with a maximum likelihood score will produce non-sparse kernel machines. Instead, we train an SVM, then train the parameters of an additional sigmoid function to map the SVM outputs into probabilities. This chapter compares classification error rate and likelihood scores for an SVM plus sigmoid versus a kernel method trained with a regularized likelihood error function. These methods are tested on three data-mining-style data sets. The SVM+sigmoid yields probabilities of comparable quality to the regularized maximum likelihood kernel method, while still retaining the sparseness of the SVM.},
author = {Platt, J},
doi = {10.1.1.41.1639},
file = {:home/maikel/Documents/mendeley/Platt/Advances in large margin classifiers/Platt{\_}1999{\_}Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.pdf:pdf},
isbn = {0262194481},
issn = {0262194481},
journal = {Advances in large margin classifiers},
mendeley-groups = {UoB/beta{\_}calibration,REFRAME/multi-class calibration workteam,UoB/bayesian{\_}calibration},
number = {3},
pages = {61--74},
pmid = {1000183100},
title = {{Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods}},
url = {https://www.researchgate.net/profile/John{\_}Platt/publication/2594015{\_}Probabilistic{\_}Outputs{\_}for{\_}Support{\_}Vector{\_}Machines{\_}and{\_}Comparisons{\_}to{\_}Regularized{\_}Likelihood{\_}Methods/links/004635154cff5262d6000000.pdf http://www.researchgate.net/profile/John{\_}Platt/publication/2594015{\_}Probabilistic{\_}Outputs{\_}for{\_}Support{\_}Vector{\_}Machines{\_}and{\_}Comparisons{\_}to{\_}Regularized{\_}Likelihood{\_}Methods/links/004635154cff5262d6000000.pdf},
volume = {10},
year = {1999}
}



@article{Buja2005,
author = {Buja, A and Stuetzle, W and Shen, Y},
file = {:home/maikel/Documents/mendeley/Buja, Stuetzle, Shen/Working draft, November/Buja, Stuetzle, Shen{\_}2005{\_}Loss functions for binary class probability estimation and classification Structure and applications.pdf:pdf},
journal = {Working draft, November},
mendeley-groups = {Artificial Neural Networks/loss functions},
title = {{Loss functions for binary class probability estimation and classification: Structure and applications}},
url = {https://pdfs.semanticscholar.org/d670/6b6e626c15680688b0774419662f2341caee.pdf},
year = {2005}
}

@inproceedings{Cid-sueiro2012,
abstract = {This paper discusses the problem of calibrating posterior class probabilities from partially labelled data. Each instance is assumed to be labelled as belonging to one of several candidate categories, at most one of them being true. We generalize the concept of proper loss to this scenario, we establish a necessary and sufﬁcient condition for a loss function to be proper, and we show a direct procedure to$\backslash$r$\backslash$nconstruct a proper loss for partial labels from a conventional proper loss. The problem can be characterized by the mixing probability matrix relating the true class of the data and the observed labels. The full knowledge of this matrix is not$\backslash$r$\backslash$nrequired, and losses can be constructed that are proper for a wide set of mixing probability matrices.},
author = {Cid-Sueiro, Jes{\'{u}}s},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/maikel/Documents/mendeley/Cid-Sueiro/Advances in Neural Information Processing Systems/Cid-Sueiro{\_}2012{\_}Proper losses for learning from partial labels.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
mendeley-groups = {UoB/week{\_}labels,Artificial Neural Networks/loss functions},
pages = {1565--1573},
title = {{Proper losses for learning from partial labels}},
url = {http://papers.nips.cc/paper/4789-proper-losses-for-learning-from-partial-labels http://books.nips.cc/papers/files/nips25/NIPS2012{\_}0738.pdf},
year = {2012}
}

@article{Cour2011,
abstract = {We address the problem of partially-labeled multiclass classification, where instead of a single label per instance, the algorithm is given a candidate set of labels, only one of which is correct. Our setting is motivated by a common scenario in many image and video collections, where only partial access to labels is available. The goal is to learn a classifier that can disambiguate the partially-labeled training instances, and generalize to unseen data. We define an intuitive property of the data distribution that sharply characterizes the ability to learn in this setting and show that effective learning is possible even when all the data is only partially labeled. Exploiting this property of the data, we propose a convex learning formulation based on minimization of a loss function appropriate for the partial label setting. We analyze the conditions under which our loss function is asymptotically consistent, as well as its generalization and transductive performance. We apply our framework to identifying faces culled from web news sources and to naming characters in TV series and movies; in particular, we annotated and experimented on a very large video data set and achieve 6{\%} error for character naming on 16 episodes of the TV series Lost.},
author = {Cour, Timothee and Sapp, Benjamin},
file = {:home/maikel/Documents/mendeley/Cour, Sapp/Journal of Machine Learning Research/Cour, Sapp{\_}2011{\_}Learning from Partial Labels.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {bounds,convex learning,generalization,multiclass classification,names faces,weakly supervised learning},
mendeley-groups = {Books},
number = {2},
pages = {1501--1536},
title = {{Learning from Partial Labels}},
url = {http://www.jmlr.org/papers/v12/cour11a.html?utm{\_}source=twitterfeed{\&}utm{\_}medium=twitter http://jmlr.csail.mit.edu/papers/v12/cour11a.html},
volume = {12},
year = {2011}
}

@article{Raykar2010,
abstract = {For many supervised learning tasks it may be infeasible (or very expensive) to obtain objective and reliable labels. Instead, we can collect subjective (possibly noisy) labels from multiple experts or annotators. In practice, there is a substantial amount of disagreement among the annotators, and hence it is of great practical interest to address conventional supervised learning problems in this scenario. In this paper we describe a probabilistic approach for supervised learning when we have multiple annotators providing (possibly noisy) labels but no absolute gold standard. The proposed algorithm evaluates the different experts and also gives an estimate of the actual hidden labels. Experimental results indicate that the proposed method is superior to the commonly used majority voting baseline.},
author = {Raykar, Vikas C. and Yu, Shipeng and Zhao, Linda H. and {Hermosillo Valadez}, Gerardo and Florin, Charles and Bogoni, Luca and Moy, Linda and Org, Linda Moy@nyumc},
doi = {10.2139/ssrn.936771},
file = {:home/maikel/Documents/mendeley/Raykar et al/Journal of Machine Learning Research/Raykar et al.{\_}2010{\_}Learning From Crowds.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {crowdsourcing,multiple annotators,multiple experts,multiple teachers},
mendeley-groups = {Data Mining/semi-supervised learning},
number = {Apr},
pages = {1297--1322},
title = {{Learning From Crowds}},
url = {http://www.jmlr.org/papers/v11/raykar10a.html},
volume = {11},
year = {2010}
}

@inproceedings{Zadrozny2002,
abstract = {Class membership probability estimates are important for many applications of data mining in which classification outputs are combined with other sources of information for decision-making, such as example-dependent misclassification costs, the outputs of other classifiers, or domain knowledge. Previous calibration methods apply only to two-class problems. Here, we show how to obtain accurate probability estimates for multiclass problems by combining calibrated binary probability estimates. We also propose a new method for obtaining calibrated two-class probability estimates that can be applied to any classifier that produces a ranking of examples. Using naive Bayes and support vector machine classifiers, we give experimental results from a variety of two-class and multiclass domains, including direct marketing, text categorization and digit recognition.},
address = {New York, New York, USA},
annote = {From Duplicate 2 (Transforming classifier scores into accurate multiclass probability estimates - Zadrozny, Bianca; Elkan, Charles)

From Duplicate 1 (Transforming classifier scores into accurate multiclass probability estimates - Zadrozny, Bianca; Elkan, Charles)

citations 2/11/2015: 361},
author = {Zadrozny, Bianca and Elkan, Charles},
booktitle = {Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '02},
doi = {10.1145/775047.775151},
file = {:home/maikel/Documents/mendeley/Zadrozny, Elkan/Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '02/Zadrozny, Elkan{\_}2002{\_}Transforming classifier scores into accurate multiclass probability estimates(2).pdf:pdf;:home/maikel/Documents/mendeley/Zadrozny, Elkan/Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '02/Zadrozny, Elkan{\_}2002{\_}Transforming classifier scores into accurate multiclass probability estimates.pdf:pdf},
isbn = {158113567X},
issn = {0340-6245},
mendeley-groups = {UoB/beta{\_}calibration,REFRAME/multi-class calibration workteam,UoB/bayesian{\_}calibration},
month = {jul},
pages = {694},
pmid = {21544309},
publisher = {ACM Press},
title = {{Transforming classifier scores into accurate multiclass probability estimates}},
url = {http://portal.acm.org/citation.cfm?doid=775047.775151 http://dl.acm.org/citation.cfm?id=775047.775151},
year = {2002}
}

@article{Guo2017,
abstract = {Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.},
archivePrefix = {arXiv},
arxivId = {1706.04599},
author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
eprint = {1706.04599},
file = {:home/maikel/Documents/mendeley/Guo et al/Unknown/Guo et al.{\_}2017{\_}On Calibration of Modern Neural Networks.pdf:pdf},
issn = {1938-7228},
mendeley-groups = {Artificial Neural Networks},
month = {jun},
title = {{On Calibration of Modern Neural Networks}},
url = {http://arxiv.org/abs/1706.04599},
year = {2017}
}


@article{Kull2017,
abstract = {For optimal decision making under variable class distributions and misclassification costs a classifier needs to produce well-calibrated estimates of the posterior probability. Isotonic calibration is a powerful non-parametric method that is however prone to overfitting on smaller datasets; hence a parametric method based on the logistic curve is commonly used. While logistic calibration is designed for normally distributed per-class scores, we demonstrate experimentally that many classifiers including Naive Bayes and Adaboost suffer from a particular distortion where these score distributions are heavily skewed. In such cases logistic calibration can easily yield probability estimates that are worse than the original scores. Moreover, the logistic curve family does not include the identity function, and hence logistic calibration can easily uncalibrate a perfectly calibrated classifier. In this paper we solve all these problems with a richer class of calibration maps based on the beta distribution. We derive the method from first principles and show that fitting it is as easy as fitting a logistic curve. Extensive experiments show that beta calibration is superior to logistic calibration for Naive Bayes and Adaboost.},
author = {Kull, Meelis and {De Menezes}, Telmo and Filho, Silva and Flach, Peter and Filho, Telmo Silva and Flach, Peter},
file = {:home/maikel/Documents/mendeley/Kull et al/Proceedings of the 20th International Conference on Artificial Intelligence and Statistics/Kull et al.{\_}2017{\_}Beta calibration a well-founded and easily implemented improvement on logistic calibration for binary classifiers.pdf:pdf},
issn = {1938-7228},
journal = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
mendeley-groups = {UoB/beta{\_}calibration},
month = {apr},
pages = {623--631},
title = {{Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers}},
url = {http://proceedings.mlr.press/v54/kull17a.html{\%}5Cnhttp://proceedings.mlr.press/v54/kull17a/kull17a.pdf},
volume = {54},
year = {2017}
}

@article{Kull2017b,
author = {Kull, Meelis and {Silva Filho}, Telmo M. and Flach, Peter},
doi = {10.1214/17-EJS1338SI},
file = {:home/maikel/Documents/mendeley/Kull, Silva Filho, Flach/Electronic Journal of Statistics/Kull, Silva Filho, Flach{\_}2017{\_}Beyond sigmoids How to obtain well-calibrated probabilities from binary classifiers with beta calibration.pdf:pdf},
issn = {1935-7524},
journal = {Electronic Journal of Statistics},
keywords = {Binary classification,beta distribution,classifier calibration,logistic function,posterior probabilities,sigmoid},
mendeley-groups = {UoB/beta{\_}calibration},
number = {2},
pages = {5052--5080},
publisher = {The Institute of Mathematical Statistics and the Bernoulli Society},
title = {{Beyond sigmoids: How to obtain well-calibrated probabilities from binary classifiers with beta calibration}},
url = {https://projecteuclid.org/euclid.ejs/1513306867},
volume = {11},
year = {2017}
}

@inproceedings{Bella2009,
author = {Bella, Antonio and Ferri, C{\`{e}}sar and Hern{\'{a}}ndez-Orallo, Jos{\'{e}} and Ram{\'{i}}rez-Quintana, Mar{\"{i}}a Jos{\'{e}}},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-04394-9_42},
file = {:home/maikel/Documents/mendeley/Bella et al/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Bella et al.{\_}2009{\_}Similarity-binning averaging A generalisation of binning calibration.pdf:pdf},
isbn = {3642043933},
issn = {03029743},
mendeley-groups = {UoB/beta{\_}calibration},
pages = {341--349},
publisher = {Springer, Berlin, Heidelberg},
title = {{Similarity-binning averaging: A generalisation of binning calibration}},
url = {http://link.springer.com/10.1007/978-3-642-04394-9{\_}42},
volume = {5788 LNCS},
year = {2009}
}


@inproceedings{Cid-Sueiro2014,
author = {Cid-Sueiro, Jesús and García-García, Darío and Santos-Rodríguez, Raúl},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-662-44848-9_13},
file = {:home/maikel/Documents/mendeley/Cid-Sueiro, Garca-Garca, Santos-Rodrguez/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Cid-Sueiro, Garca-Garca, Santos-Rodrguez{\_}2014{\_}Consistency of losses for learning from weak labels.pdf:pdf},
isbn = {9783662448472},
issn = {16113349},
mendeley-groups = {UoB/activity recognition,UoB},
number = {PART 1},
pages = {197--210},
publisher = {Springer Berlin Heidelberg},
title = {{Consistency of losses for learning from weak labels}},
url = {http://link.springer.com/10.1007/978-3-662-44848-9{\_}13},
volume = {8724 LNAI},
year = {2014}
}

@misc{Lichman:2013 ,
author = "M. Lichman",
year = "2013",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@inproceedings{Hullermeier2015,
abstract = {In standard supervised learning, each training instance is associated with an outcome from a corresponding output space (eg, a class label in classification or a real number in regression). In the superset learning problem, the outcome is only characterized in terms of a superset—a subset of candidates that covers the true outcome but may also contain additional ones. Thus, superset learning can be seen as a specific type of weakly supervised learning, in which training examples are ambiguous. In this paper, we introduce a generic approach to superset learning, which is motivated by the idea of performing model identification and “data disambiguation” simultaneously. This idea is realized by means of a generalized risk minimization approach, using an extended loss function that compares precise predictions with set-valued observations. As an illustration, we instantiate our meta learning technique for the problem of label ranking, in which the output space consists of all permutations of a fixed set of items. The label ranking method thus obtained is compared to existing approaches tackling the same problem.},
author = {H{\"{u}}llermeier, Eyke and Cheng, Weiwei},
booktitle = {Machine learning and Knowledge Discovery in Databases},
doi = {10.1007/978-3-319-23525-7_16},
file = {:home/maikel/Documents/mendeley/H{\"{u}}llermeier, Cheng/Machine learning and Knowledge Discovery in Databases/H{\"{u}}llermeier, Cheng{\_}2015{\_}Superset Learning Based on Generalized Loss Minimization.pdf:pdf},
keywords = {Amico,Entropy,Smit,Summing},
mendeley-groups = {UoB/week{\_}labels},
pages = {260--275},
publisher = {Springer, Cham},
title = {{Superset Learning Based on Generalized Loss Minimization}},
url = {http://link.springer.com/10.1007/978-3-319-23525-7{\_}16},
year = {2015}
}

@article{Hung1996,
author = {Hung, MS and Hu, MY and Shanker, MS and Patuwo, BE},
file = {:home/maikel/Documents/mendeley/Hung et al/International Journal of Computational Intelligence and Organizations/Hung et al.{\_}1996{\_}Estimating posterior probabilities in classification problems with neural networks.pdf:pdf},
journal = {International Journal of Computational Intelligence and Organizations},
mendeley-groups = {REFRAME/multi-class calibration workteam,UoB/bayesian{\_}calibration},
number = {1},
pages = {49--60},
title = {{Estimating posterior probabilities in classification problems with neural networks}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.329.7337{\&}rep=rep1{\&}type=pdf},
volume = {1},
year = {1996}
}

@article{zhang2000,
  title={Neural networks for classification: a survey},
  author={Zhang, Guoqiang Peter},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume={30},
  number={4},
  pages={451--462},
  year={2000},
  publisher={IEEE}
}

@article{Richard1991,
abstract = {Many neural network classifiers provide outputs which estimate Bayesian a posteriori probabilities. When the estimation is accurate, network outputs can be treated as probabilities and sum to one. Simple proofs show that Bayesian probabilities are estimated when desired network outputs are 1 of M (one output unity, all others zero) and a squared-error or cross-entropy cost function is used. Results of Monte Carlo simulations performed using multilayer perceptron (MLP) networks trained with backpropagation, radial basis function (RBF) networks, and high-order polynomial networks graphically demonstrate that network outputs provide good estimates of Bayesian probabilities. Estimation accuracy depends on network complexity, the amount of training data, and the degree to which training data reflect true likelihood distributions and a priori class probabilities. Interpretation of network outputs as Bayesian probabilities allows outputs from multiple networks to be combined for higher level decision making, sim...},
author = {Richard, Michael D. and Lippmann, Richard P.},
doi = {10.1162/neco.1991.3.4.461},
file = {:home/maikel/Documents/mendeley/Richard, Lippmann/Neural Computation/Richard, Lippmann{\_}1991{\_}Neural Network Classifiers Estimate Bayesian a posteriori Probabilities.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
language = {en},
mendeley-groups = {REFRAME/multi-class calibration workteam},
month = {dec},
number = {4},
pages = {461--483},
publisher = {MIT Press 238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu},
title = {{Neural Network Classifiers Estimate Bayesian a posteriori Probabilities}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.1991.3.4.461{\#}.VnQq{\_}F4c1Qs},
volume = {3},
year = {1991}
}

@article{Landgrebe,
author = {Landgrebe, Thomas C. W. and Tax, David M. J. and Pacl{\'{i}}k, Pavel and Duin, Robert P. W. and Andrew, Colin},
file = {:home/maikel/Documents/mendeley/Landgrebe et al/Unknown/Landgrebe et al.{\_}Unknown{\_}A combining strategy for ill-defined problems.pdf:pdf},
mendeley-groups = {REFRAME/rejection},
title = {{A combining strategy for ill-defined problems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.132.1111}
}

@article{Grunwald2017,
abstract = {We formalize the idea of probability distributions that lead to reliable predictions about some, but not all aspects of a domain. The resulting notion of 'safety' provides a fresh perspective on foundational issues in statistics, providing a middle ground between imprecise probability and multiple-prior models on the one hand and strictly Bayesian approaches on the other. It also allows us to formalize fiducial distributions in terms of the set of random variables that they can safely predict, thus taking some of the sting out of the fiducial idea. By restricting probabilistic inference to safe uses, one also automatically avoids paradoxes such as the Monty Hall problem. Safety comes in a variety of degrees, such as 'validity' (the strongest notion), 'calibration', 'confidence safety' and 'unbiasedness' (almost the weakest notion).},
archivePrefix = {arXiv},
arxivId = {1604.01785},
author = {Gr{\"{u}}nwald, Peter},
doi = {10.1016/j.jspi.2017.09.014},
eprint = {1604.01785},
file = {:home/maikel/Documents/mendeley/Gr{\"{u}}nwald/Unknown/Gr{\"{u}}nwald{\_}2016{\_}Safe Probability.pdf:pdf},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
mendeley-groups = {REFRAME/rejection},
month = {apr},
title = {{Safe probability}},
url = {http://arxiv.org/abs/1604.01785},
year = {2017}
}

@article{DeFaria2015,
author = {de Faria, Elaine Ribeiro and {Ponce de Leon Ferreira Carvalho}, Andr{\'{e}} Carlos and Gama, Jo{\~{a}}o},
doi = {10.1007/s10618-015-0433-y},
file = {:home/maikel/Documents/mendeley/de Faria, Ponce de Leon Ferreira Carvalho, Gama/Data Mining and Knowledge Discovery/de Faria, Ponce de Leon Ferreira Carvalho, Gama{\_}2015{\_}MINAS multiclass learning algorithm for novelty detection in data streams.pdf:pdf},
issn = {1384-5810},
journal = {Data Mining and Knowledge Discovery},
mendeley-groups = {REFRAME/rejection},
month = {aug},
number = {3},
pages = {640--680},
title = {{MINAS: multiclass learning algorithm for novelty detection in data streams}},
url = {http://link.springer.com/10.1007/s10618-015-0433-y},
volume = {30},
year = {2015}
}


@inproceedings{Lasserre2006,
abstract = {When labelled training data is plentiful, discriminative techniques are widely used since they give excellent generalization performance. However, for large-scale applications such as object recognition, hand labelling of data is expensive, and there is much interest in semi-supervised techniques based on generative models in which the majority of the training data is unlabelled. Although the generalization performance of generative models can often be improved by {\&}{\#}145;training them discriminatively{\&}{\#}146;, they can then no longer make use of unlabelled data. In an attempt to gain the benefit of both generative and discriminative approaches, heuristic procedure have been proposed [2, 3] which interpolate between these two extremes by taking a convex combination of the generative and discriminative objective functions. In this paper we adopt a new perspective which says that there is only one correct way to train a given model, and that a {\&}{\#}145;discriminatively trained{\&}{\#}146; generative model is fundamentally a new model [7]. From this viewpoint, generative and discriminative models correspond to specific choices for the prior over parameters. As well as giving a principled interpretation of {\&}{\#}145;discriminative training{\&}{\#}146;, this approach opens door to very general ways of interpolating between generative and discriminative extremes through alternative choices of prior. We illustrate this framework using both synthetic data and a practical example in the domain of multi-class object recognition. Our results show that, when the supply of labelled training data is limited, the optimum performance corresponds to a balance between the purely generative and the purely discriminative.},
author = {Lasserre, J.A. and Bishop, C.M. and Minka, T.P.},
booktitle = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1 (CVPR'06)},
doi = {10.1109/CVPR.2006.227},
file = {:home/maikel/Documents/mendeley/Lasserre, Bishop, Minka/2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1 (CVPR'06)/Lasserre, Bishop, Minka{\_}2006{\_}Principled Hybrids of Generative and Discriminative Models.pdf:pdf},
isbn = {0-7695-2597-0},
issn = {1063-6919},
keywords = {Application software,Computer vision,Hybrid power systems,Labeling,Large-scale systems,Machine learning,Object recognition,Parametric statistics,Predictive models,Training data},
mendeley-groups = {REFRAME/rejection},
pages = {87--94},
publisher = {IEEE},
shorttitle = {Computer Vision and Pattern Recognition, 2006 IEEE},
title = {{Principled Hybrids of Generative and Discriminative Models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1640745},
volume = {1},
year = {2006}
}

@incollection{Bishop2007,
author = {Bishop, Christopher and Lasserre, Julia},
booktitle = {Bayesian Statistics},
editor = {Bernardo, JM and Bayarri, MJ and Berger, JO and Dawid, AP and Heckerman, D and Smith, AFM and West, M},
file = {:home/maikel/Documents/mendeley/Bishop, Lasserre/Bayesian Statistics/Bishop, Lasserre{\_}2007{\_}Generative or Discriminative Getting the Best of Both Worlds.pdf:pdf},
mendeley-groups = {REFRAME/rejection},
pages = {3 -- 24},
title = {{Generative or Discriminative? Getting the Best of Both Worlds}},
volume = {8},
year = {2007}
}

@incollection{Hempstalk2008,
abstract = {One-class classification has important applications such as outlier and novelty detection. It is commonly tackled using density estimation techniques or by adapting a standard classification algorithm to the problem of carving out a decision boundary that describes the location of the target data. In this paper we investigate a simple method for one-class classification that combines the application of a density estimator, used to form a reference distribution, with the induction of a standard model for class probability estimation. In this method, the reference distribution is used to generate artificial data that is employed to form a second, artificial class. In conjunction with the target class, this artificial class is the basis for a standard two-class learning problem. We explain how the density function of the reference distribution can be combined with the class probability estimates obtained in this way to form an adjusted estimate of the density function of the target class. Using UCI datasets, and data from a typist recognition problem, we show that the combined model, consisting of both a density estimator and a class probability estimator, can improve on using either component technique alone when used for one-class classification. We also compare the method to one-class classification using support vector machines.},
address = {Berlin, Heidelberg},
author = {Hempstalk, Kathryn and Frank, Eibe and Witten, Ian H.},
booktitle = {Machine Learning and Knowledge Discovery in Databases},
doi = {10.1007/978-3-540-87479-9},
editor = {Daelemans, Walter and Goethals, Bart and Morik, Katharina},
file = {:home/maikel/Documents/mendeley/Hempstalk, Frank, Witten/Machine Learning and Knowledge Discovery in Databases/Hempstalk, Frank, Witten{\_}2008{\_}One-Class Classification by Combining Density and Class Probability Estimation.pdf:pdf},
isbn = {978-3-540-87478-2},
issn = {0302-9743},
mendeley-groups = {REFRAME/rejection},
pages = {505--519},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{One-Class Classification by Combining Density and Class Probability Estimation}},
url = {http://www.springerlink.com/index/10.1007/978-3-540-87479-9},
volume = {5211},
year = {2008}
}

@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
doi = {10.1017/CBO9781139058452},
eprint = {arXiv:1406.2661v1},
file = {:home/maikel/Documents/mendeley/Goodfellow et al/Advances in Neural Information Processing Systems 27/Goodfellow et al.{\_}2014{\_}Generative Adversarial Nets.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27},
pages = {2672--2680},
pmid = {1000183096},
title = {{Generative Adversarial Nets}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
year = {2014}
}

@article{Khan2014,
author = {Khan, Shehroz S. and Madden, Michael G.},
doi = {10.1017/S026988891300043X},
file = {:home/maikel/Documents/mendeley/Khan, Madden/The Knowledge Engineering Review/Khan, Madden{\_}2014{\_}One-class classification taxonomy of study and review of techniques.pdf:pdf},
issn = {0269-8889},
journal = {The Knowledge Engineering Review},
language = {English},
mendeley-groups = {REFRAME/rejection},
month = {jan},
number = {03},
pages = {345--374},
publisher = {Cambridge University Press},
title = {{One-class classification: taxonomy of study and review of techniques}},
url = {http://journals.cambridge.org/abstract{\_}S026988891300043X},
volume = {29},
year = {2014}
}

@article{Markou2003,
abstract = {Novelty detection is the identification of new or unknown data or signal that a machine learning system is not aware of during training. Novelty detection is one of the fundamental requirements of a good classification or identification system since sometimes the test data contains information about objects that were not known at the time of training the model. In this paper we provide state-of-the-art review in the area of novelty detection based on statistical approaches. The second part paper details novelty detection using neural networks. As discussed, there are a multitude of applications where novelty detection is extremely important including signal processing, computer vision, pattern recognition, data mining, and robotics.},
author = {Markou, Markos and Singh, Sameer},
doi = {10.1016/j.sigpro.2003.07.018},
file = {:home/maikel/Documents/mendeley/Markou, Singh/Signal Processing/Markou, Singh{\_}2003{\_}Novelty detection a review - part 1 statistical approaches.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Clustering,Gaussian mixture models,Hidden Markov models,KNN,Novelty detection review,Parzen density estimation,Statistical approaches,String matching},
mendeley-groups = {REFRAME/rejection},
month = {dec},
number = {12},
pages = {2481--2497},
title = {{Novelty detection: a review - part 1: statistical approaches}},
url = {http://www.sciencedirect.com/science/article/pii/S0165168403002020},
volume = {83},
year = {2003}
}

@article{Markou2003a,
abstract = {Novelty detection is the identification of new or unknown data or signal that a machine learning system is not aware of during training. In this paper we focus on neural network-based approaches for novelty detection. Statistical approaches are covered in Part 1 paper.},
author = {Markou, Markos and Singh, Sameer},
doi = {10.1016/j.sigpro.2003.07.019},
file = {:home/maikel/Documents/mendeley/Markou, Singh/Signal Processing/Markou, Singh{\_}2003{\_}Novelty detection a review- part 2.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {ART,MLP,Network-based approaches,Neural networks,Novelty detection,RBF},
mendeley-groups = {REFRAME/rejection},
month = {dec},
number = {12},
pages = {2499--2521},
title = {{Novelty detection: a review- part 2:}},
url = {http://www.sciencedirect.com/science/article/pii/S0165168403002032},
volume = {83},
year = {2003}
}

@inproceedings{Ferri2004,
author = {Ferri, C and Hern{\'{a}}ndez-Orallo, J},
booktitle = {Proceedings of ROC Analysis in Artificial Intelligence, 1st International Workshop (ROCAI-2004)},
file = {:home/maikel/Documents/mendeley/Ferri, Hern{\'{a}}ndez-Orallo/Proceedings of ROC Analysis in Artificial Intelligence, 1st International Workshop (ROCAI-2004)/Ferri, Hern{\'{a}}ndez-Orallo{\_}2004{\_}Cautious Classifiers.pdf:pdf},
mendeley-groups = {REFRAME/rejection},
pages = {27--36},
title = {{Cautious Classifiers.}},
url = {http://www.dsic.upv.es/{~}flip/ROCAI2004/papers/04-ROCAI2004-Ferri-HdezOrallo.pdf},
year = {2004}
}

@article{Landgrebe2006,
abstract = {Consider the class of problems in which a target class is well-defined, and an outlier class is ill-defined. In these cases new outlier classes can appear, or the class-conditional distribution of the outlier class itself may be poorly sampled. A strategy to deal with this problem involves a two-stage classifier, in which one stage is designed to perform discrimination between known classes, and the other stage encloses known data to protect against changing conditions. The two stages are, however, interrelated, implying that optimising one may compromise the other. In this paper the relation between the two stages is studied within an ROC analysis framework. We show how the operating characteristics can be used for both model selection, and in aiding in the choice of the reject threshold. An analytic study on a controlled experiment is performed, followed by some experiments on real-world datasets with the distance-based reject-option classifier.},
author = {Landgrebe, Thomas C.W. and Tax, David M.J. and Pacl{\'{i}}k, Pavel and Duin, Robert P.W.},
doi = {10.1016/j.patrec.2005.10.015},
file = {:home/maikel/Documents/mendeley/Landgrebe et al/Pattern Recognition Letters/Landgrebe et al.{\_}2006{\_}The interaction between classification and reject performance for distance-based reject-option classifiers.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Ill-defined classification problems,Model selection,ROC analysis,Reject-option,Unseen classes},
mendeley-groups = {REFRAME/rejection},
month = {jun},
number = {8},
pages = {908--917},
title = {{The interaction between classification and reject performance for distance-based reject-option classifiers}},
url = {http://www.sciencedirect.com/science/article/pii/S0167865505003089},
volume = {27},
year = {2006}
}

@article{Chow1970,
abstract = {The performance of a pattern recognition system is characterized by its error and reject tradeoff. This paper describes an optimum rejection rule and presents a general relation between the error and reject probabilities and some simple properties of the tradeoff in the optimum recognition system. The error rate can be directly evaluated from the reject function. Some practical implications of the results are discussed. Examples in normal distributions and uniform distributions are given.},
author = {Chow, C. K.},
doi = {10.1109/TIT.1970.1054406},
issn = {15579654},
journal = {IEEE Transactions on Information Theory},
mendeley-groups = {REFRAME/rejection},
month = {jan},
number = {1},
pages = {41--46},
title = {{On Optimum Recognition Error and Reject Tradeoff}},
url = {http://ieeexplore.ieee.org/document/1054406/},
volume = {16},
year = {1970}
}

@inproceedings{Niculescu-Mizil2005,
address = {New York, New York, USA},
author = {Niculescu-Mizil, Alexandru and Caruana, Rich},
booktitle = {Proceedings of the 22nd international conference on Machine learning - ICML '05},
doi = {10.1145/1102351.1102430},
file = {:home/maikel/Documents/mendeley/Niculescu-Mizil, Caruana/Proceedings of the 22nd international conference on Machine learning - ICML '05/Niculescu-Mizil, Caruana{\_}2005{\_}Predicting good probabilities with supervised learning.pdf:pdf},
isbn = {1595931805},
mendeley-groups = {REFRAME/multi-class calibration workteam,UoB/bayesian{\_}calibration},
month = {aug},
pages = {625--632},
publisher = {ACM Press},
title = {{Predicting good probabilities with supervised learning}},
url = {http://dl.acm.org/citation.cfm?id=1102351.1102430},
year = {2005}
}

@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.   The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {:home/maikel/Documents/mendeley/He et al/Unknown/He et al.{\_}2015{\_}Deep Residual Learning for Image Recognition.pdf:pdf},
mendeley-groups = {Artificial Neural Networks/Deep Learning},
month = {dec},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}

@inproceedings{huang2016,
  title={Deep networks with stochastic depth},
  author={Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q},
  booktitle={European Conference on Computer Vision},
  pages={646--661},
  year={2016},
  organization={Springer}
}

@inproceedings{huang2017,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q and van der Maaten, Laurens},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  volume={1},
  number={2},
  pages={3},
  year={2017}
}


% to be cited

@article{Goodfellow2014a,
abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples--inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
eprint = {1412.6572},
file = {:home/maikel/Documents/mendeley/Goodfellow, Shlens, Szegedy/arXiv1412.6572 cs, stat/Goodfellow, Shlens, Szegedy{\_}2014{\_}Explaining and Harnessing Adversarial Examples.pdf:pdf},
journal = {arXiv:1412.6572 [cs, stat]},
keywords = {Computer Science - Learning,Statistics - Machine Learning},
mendeley-groups = {Zotero - My Collection,REFRAME/multi-class calibration workteam,Read pending,Zotero - Zotero Library,Artificial Neural Networks/Convolutional Neural Networks/visualization},
mendeley-tags = {Computer Science - Learning,Statistics - Machine Learning},
month = {dec},
title = {{Explaining and Harnessing Adversarial Examples}},
url = {http://arxiv.org/abs/1412.6572},
year = {2014}
}

@inproceedings{Kull2014,
abstract = {We propose a general method to assess the reliability of two-class probabilities in an instance-wise manner. This is relevant, for instance, for obtaining calibrated multi-class probabilities from two-class probability scores. The LS-ECOC method approaches this by performing least-squares fitting over a suitable error-correcting output code matrix, where the optimisation resolves potential conflicts in the input probabilities. While this gives all input probabilities equal weight, we would like to spend less effort fitting unreliable probability estimates. We introduce the concept of a reliability map to accompany the more conventional notion of calibration map; and LS-ECOC-R which modifies LS-ECOC to take reliability into account. We demonstrate on synthetic data that this gets us closer to the Bayes-optimal classifier, even if the base classifiers are linear and hence have high bias. Results on UCI data sets demonstrate that multi-class accuracy also improves.},
author = {Kull, Meelis and Flach, Peter A.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-662-44851-9_2},
file = {:home/maikel/Documents/mendeley/Kull, Flach/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Kull, Flach{\_}2014{\_}Reliability maps A tool to enhance probability estimates and improve classification accuracy.pdf:pdf},
isbn = {9783662448502},
issn = {16113349},
mendeley-groups = {UoB/bayesian{\_}calibration},
number = {PART 2},
pages = {18--33},
publisher = {Springer Berlin Heidelberg},
title = {{Reliability maps: A tool to enhance probability estimates and improve classification accuracy}},
url = {http://link.springer.com/10.1007/978-3-662-44851-9{\_}2},
volume = {8725 LNAI},
year = {2014}
}

@incollection{Weston2012,
abstract = {We show how nonlinear semi-supervised embedding algorithms popular for use with “shallow” learning techniques such as kernel methods can be easily applied to deep multi-layer architectures, either as a regularizer at the output layer, or on each layer of the architecture. Compared to standard supervised backpropagation this can give significant gains. This trick provides a simple alternative to existing approaches to semi-supervised deep learning whilst yielding competitive error rates compared to those methods, and existing shallow semi-supervised techniques.},
author = {Weston, Jason and Ratle, Fr{\'{e}}d{\'{e}}ric and Mobahi, Hossein and Collobert, Ronan},
editor = {Montavon, Gr{\'{e}}goire and Orr, Genevi{\`{e}}ve B. and M{\"{u}}ller, Klaus-Robert},
file = {:home/maikel/Documents/mendeley/Weston et al/Unknown/Weston et al.{\_}2012{\_}Deep Learning via Semi-supervised Embedding.pdf:pdf;:home/maikel/Documents/mendeley/Weston et al/Unknown/Weston et al.{\_}2012{\_}Deep Learning via Semi-supervised Embedding.html:html},
isbn = {978-3-642-35288-1, 978-3-642-35289-8},
keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),Pattern Recognition},
language = {en},
mendeley-groups = {Zotero - My Collection,Zotero - Zotero Library,Artificial Neural Networks/Deep Learning},
mendeley-tags = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),Pattern Recognition},
month = {jan},
pages = {639--655},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Deep Learning via Semi-supervised Embedding}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-35289-8{\_}34 http://link.springer.com/content/pdf/10.1007{\%}2F978-3-642-35289-8{\_}34.pdf},
year = {2012}
}

@article{Rasmus2015,
abstract = {We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pretraining. Our work builds on top of the Ladder network proposed by Valpola (2015) which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in various tasks: MNIST and CIFAR-10 classification in a semi-supervised setting and permutation invariant MNIST in both semi-supervised and full-labels setting.},
archivePrefix = {arXiv},
arxivId = {arxiv:1507.02672},
author = {Rasmus, Antti and Valpola, Harri and Honkala, Mikko and Berglund, Mathias and Raiko, Tapani},
eprint = {arxiv:1507.02672},
file = {:home/maikel/Documents/mendeley/Rasmus et al/arxiv/Rasmus et al.{\_}2015{\_}Semi-Supervised Learning with Ladder Network.pdf:pdf;:home/maikel/Documents/mendeley/Rasmus et al/arxiv/Rasmus et al.{\_}2015{\_}Semi-Supervised Learning with Ladder Network.html:html},
journal = {arxiv},
keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computi,Statistics - Machine Learning},
mendeley-groups = {Zotero - My Collection,Zotero - Zotero Library},
mendeley-tags = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computi,Statistics - Machine Learning},
month = {jul},
title = {{Semi-Supervised Learning with Ladder Network}},
url = {http://arxiv.org/abs/1507.02672 http://www.arxiv.org/pdf/1507.02672.pdf},
year = {2015}
}

@article{Bengio2012,
author = {Bengio, Y},
file = {:home/maikel/Documents/mendeley/Bengio/Unsupervised and Transfer Learning Challenges in {\ldots}/Bengio{\_}2012{\_}Deep learning of representations for unsupervised and transfer learning.pdf:pdf;:home/maikel/Documents/mendeley/Bengio/Unsupervised and Transfer Learning Challenges in {\ldots}/Bengio{\_}2012{\_}Deep learning of representations for unsupervised and transfer learning(2).pdf:pdf},
journal = {Unsupervised and Transfer Learning Challenges in {\ldots}},
mendeley-groups = {Computer Vision/videos/transfer learning},
title = {{Deep learning of representations for unsupervised and transfer learning}},
url = {http://feed.mtome.com/Publications/CiML/CiML-v7-book.pdf{\#}page=29},
year = {2012}
}


@article{Lee2013,
abstract = {We propose the simple and efficient method of semi-supervised learning for deep neural networks. Basically, the proposed network is trained in a supervised fashion with labeled and unlabeled data simultaneously. For un-labeled data, Pseudo-Labels, just picking up the class which has the maximum predicted probability, are used as if they were true labels. This is in effect equivalent to Entropy Regularization. It favors a low-density separation between classes, a commonly assumed prior for semi-supervised learning. With Denoising Auto-Encoder and Dropout, this simple method outperforms conventional methods for semi-supervised learning with very small labeled data on the MNIST handwritten digit dataset.},
author = {Lee, Dong-Hyun},
file = {:home/maikel/Documents/mendeley/Lee/ICML 2013 Workshop Challenges in Representation Learning/Lee{\_}2013{\_}Pseudo-label The simple and efficient semi-supervised learning method for deep neural networks.pdf:pdf},
journal = {ICML 2013 Workshop: Challenges in Representation Learning},
mendeley-groups = {Books},
pages = {1--6},
title = {{Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks}},
url = {https://www.kaggle.com/blobs/download/forum-message-attachment-files/746/pseudo{\_}label{\_}final.pdf},
year = {2013}
}
